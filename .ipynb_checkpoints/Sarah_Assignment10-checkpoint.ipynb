{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88dbaf3",
   "metadata": {
    "id": "b88dbaf3"
   },
   "source": [
    "# Assignment 10 – Applied Component\n",
    "Topic: Model Explainability, Fairness, and Interpretability in Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fac144",
   "metadata": {
    "id": "f7fac144"
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Theoretical Background\n",
    "\n",
    "### 1.1 Logistic Regression\n",
    "\n",
    "Logistic Regression is used for binary classification tasks and models the probability of a given input belonging to a specific class. It uses the logistic (sigmoid) function to map any real-valued number into a range between 0 and 1:\n",
    "\n",
    "$$\n",
    "\\displaystyle\n",
    "P(y = 1 \\mid X) \\;=\\; \\frac{1}{\\,1 + e^{-(\\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\cdots + \\beta_{n}X_{n})}\\,}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Equivalently, in compact summation form:\n",
    "\n",
    "\n",
    "$$\n",
    "\\displaystyle\n",
    "P(y = 1 \\mid X) \\;=\\; \\frac{1}{\\,1 + e^{-(\\beta_{0} + \\sum_{j=1}^{n} \\beta_{j} X_{j})}\\,}\n",
    "$$\n",
    "\n",
    "The model is trained using **maximum likelihood estimation (MLE)**, which finds the parameters (\\(\\beta\\)) that maximize the probability of observing the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452c5c54",
   "metadata": {
    "id": "452c5c54"
   },
   "source": [
    "### 1.2 XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "XGBoost is an ensemble learning algorithm based on gradient boosting. It builds trees sequentially, where each tree corrects the errors of the previous one.\n",
    "\n",
    "The objective function combines a loss term and a regularization term:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ff446",
   "metadata": {
    "id": "f24ff446"
   },
   "source": [
    "$$\n",
    "\\displaystyle\n",
    "\\mathrm{Obj} \\;=\\; \\sum_{i} \\ell\\!\\left(y_i, \\hat{y}_i\\right) \\;+\\; \\sum_{k} \\Omega\\!\\left(f_k\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb34d7",
   "metadata": {
    "id": "9cdb34d7"
   },
   "source": [
    "with regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70529c",
   "metadata": {
    "id": "ef70529c"
   },
   "source": [
    "$$\n",
    "\\displaystyle\n",
    "\\Omega\\!\\left(f_k\\right) \\;=\\; \\gamma\\,T \\;+\\; \\tfrac{1}{2}\\,\\lambda\\,\\lVert w \\rVert^{2}\n",
    "$$\n",
    "\n",
    "Here, \\(T\\) is the number of leaves and \\(w\\) are the leaf weights. Regularization helps prevent overfitting. This makes XGBoost powerful yet regularized to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5b8b6",
   "metadata": {
    "id": "ccd5b8b6"
   },
   "source": [
    "### 1.3 Explainability with SHAP\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) assigns each feature an importance value for a particular prediction. It is based on cooperative game theory, where feature contributions are computed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0108c558",
   "metadata": {
    "id": "0108c558"
   },
   "source": [
    "$$\n",
    "\\displaystyle\n",
    "\\phi_i \\;=\\; \\sum_{S \\subseteq F \\setminus \\{i\\}}\\; \\frac{|S|!\\,\\left(|F|-|S|-1\\right)!}{|F|!}\\;\\Big[\\, f(S \\cup \\{i\\}) \\;-\\; f(S) \\,\\Big]\n",
    "$$\n",
    "\n",
    "This helps in explaining how each input variable influences the model’s output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NoKouMUXgmlT",
   "metadata": {
    "id": "NoKouMUXgmlT"
   },
   "source": [
    "# Step 1 – Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d01737-89ec-490c-b6e0-e23fd746a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wX_Hpv1Sgpct",
   "metadata": {
    "id": "wX_Hpv1Sgpct"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Snfcu-P8gvQe",
   "metadata": {
    "id": "Snfcu-P8gvQe"
   },
   "source": [
    "#Step 2 – Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8grW2keEgw_-",
   "metadata": {
    "id": "8grW2keEgw_-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# Detect the correct file encoding\n",
    "with open(r\"C:\\Users\\sarah\\Desktop\\BC\\Fall 2025\\MESA8414 Applied AI and Machine Learning\\Assignment\\Week 10 assignment\\IST dataset supp1 (1) trials 2011.csv\", 'rb') as f:                 # FILL IN ALL BLANKS\n",
    "    result = chardet.detect(f.read(100000))\n",
    "\n",
    "# Load dataset safely with detected encoding\n",
    "df = pd.read_csv(r\"C:\\Users\\sarah\\Desktop\\BC\\Fall 2025\\MESA8414 Applied AI and Machine Learning\\Assignment\\Week 10 assignment\\IST dataset supp1 (1) trials 2011.csv\", encoding=result['encoding'], low_memory=False)\n",
    "\n",
    "print(\"File loaded successfully!\")\n",
    "print(\"Detected encoding:\", result['encoding'])\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "print(\"\\nPreview of data:\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QQUZAuXGimj_",
   "metadata": {
    "id": "QQUZAuXGimj_"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\\n\", df.isnull().sum())    # FILL IN ALL BLANKS\n",
    "\n",
    "# Handle missing data\n",
    "df = df.fillna(df.mean(numeric_only=True))\n",
    "\n",
    "display(df.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ktFMAZpi1Ylh",
   "metadata": {
    "id": "ktFMAZpi1Ylh"
   },
   "source": [
    "Interpretation Question:\n",
    "What would be the possible implications of not handling missing or inconsistent values in this dataset before training predictive models like Logistic Regression or XGBoost?\n",
    "\n",
    "* The analysts need to first consider whether the missing values are random. If there are clear reasons or patterns in which data missing appears, it could lead to biased findings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cB-wLKAMipSW",
   "metadata": {
    "id": "cB-wLKAMipSW"
   },
   "source": [
    "# Step 3 – Feature Correlation and Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lvxn6dTMiosF",
   "metadata": {
    "id": "lvxn6dTMiosF"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Available columns:\\n\", df.columns.tolist())  # FILL IN ALL BLANKS\n",
    "\n",
    "# Identify the likely target column\n",
    "possible_targets = [col for col in df.columns if 'DDIAGISC' in col.upper() or 'DDIAGHA' in col.upper() or 'DDIAGUN' in col.upper()]\n",
    "print(\"\\nPossible target columns:\", possible_targets)\n",
    "\n",
    "df.rename(columns={'DDIAGISC': 'Outcome'}, inplace=True)\n",
    "\n",
    "#visualize\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(x='Outcome', data=df)\n",
    "plt.title(\"Outcome Distribution (0 = No Stroke, 1 = Stroke)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ef9140-fd28-4971-9a9d-601ef6c24779",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DbtrjUPijnZD",
   "metadata": {
    "id": "DbtrjUPijnZD"
   },
   "source": [
    "#Step 4 – Split Data and Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bOXCMUzCwFOc",
   "metadata": {
    "id": "bOXCMUzCwFOc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Drop obvious leakage columns\n",
    "leakage_cols = [col for col in df.columns if 'DEAD1' in col or 'DEAD2' in col or 'DEAD3' in col]    # FILL IN ALL BLANKS\n",
    "print(\"Dropping leakage columns:\", leakage_cols)\n",
    "X = df.drop(columns=leakage_cols + ['Outcome'])\n",
    "y = df['Outcome']\n",
    "\n",
    "# Encode categorical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object' or X[col].dtype == 'bool':\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "X = X.fillna(0)\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=30\n",
    ")\n",
    "\n",
    "# Balance classes with SMOTE\n",
    "sm = SMOTE(random_state=30)\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train )\n",
    "print(\"After SMOTE:\", y_train_bal.value_counts().to_dict())\n",
    "\n",
    "# Scale for logistic regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_bal)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dM_2Avjl1wOR",
   "metadata": {
    "id": "dM_2Avjl1wOR"
   },
   "source": [
    "Interpretation Question:\n",
    "Why is it crucial to encode categorical variables before scaling, and what might occur if categorical features remain unencoded during the training of these models?\n",
    "\n",
    "* It's important to encode categorical variables before scaling because scalers require data to be in a numeric format. Without scaling, you would run into errors training the datset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nqJAwbKPkLfR",
   "metadata": {
    "id": "nqJAwbKPkLfR"
   },
   "source": [
    "#Step 5 – Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6G7BS3YvkQem",
   "metadata": {
    "id": "6G7BS3YvkQem"
   },
   "outputs": [],
   "source": [
    "# Retrain Logistic Regression with SMOTE-balanced data\n",
    "log_model = LogisticRegression(max_iter=1000, random_state=30)    # FILL IN ALL BLANKS\n",
    "\n",
    "\n",
    "log_model.fit(X_train_scaled, y_train_bal)\n",
    "\n",
    "y_pred_lr = log_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"-- Logistic Regression Report --\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, log_model.predict_proba(X_test_scaled)[:,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kQN238D_11eQ",
   "metadata": {
    "id": "kQN238D_11eQ"
   },
   "source": [
    "Interpretation Question:\n",
    "What can we infer about feature relationships from the magnitude and sign of Logistic Regression coefficients, and how does this aid interpretability in a clinical setting?\n",
    "\n",
    "* The magnitude of the coefficients indicates stronger relationships. The positive sign indicates higher log-odds and the negative sign indicates lower log-odds or the outcome.\n",
    "* In a clinical setting, you can use the logistic regression coefficients to compare across different causes, diagnosis, and interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5V6yBW3HkWIM",
   "metadata": {
    "id": "5V6yBW3HkWIM"
   },
   "source": [
    "#Step 6 – XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YQPAHWTTkTXP",
   "metadata": {
    "id": "YQPAHWTTkTXP"
   },
   "outputs": [],
   "source": [
    "# Initialize XGBoost classifier\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,         # FILL IN ALL BLANKS\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=30,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"-- XGBoost Model Report --\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, xgb_model.predict_proba(X_test)[:, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RiLiw7FQ17ya",
   "metadata": {
    "id": "RiLiw7FQ17ya"
   },
   "source": [
    "Interpretation Question:\n",
    "In comparison to Logistic Regression, how does XGBoost’s gradient-boosting mechanism influence the way it learns patterns from the same dataset?\n",
    "\n",
    "* While a logistic regression uses a single equation to fit the entire dataset, XGBoost builds a series of decision trees that learn patterns iteratively and nonlinearly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fzzwa8BZkiyf",
   "metadata": {
    "id": "Fzzwa8BZkiyf"
   },
   "source": [
    "#Step 7 – Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FVFoHlyYkj91",
   "metadata": {
    "id": "FVFoHlyYkj91"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))   # FILL IN ALL BLANKS\n",
    "\n",
    "# Plot ROC curves for both models\n",
    "RocCurveDisplay.from_estimator(log_model, X_test_scaled, y_test, ax=plt.gca(), name=\"Logistic Regression\")\n",
    "RocCurveDisplay.from_estimator(xgb_model, X_test, y_test, ax=plt.gca(), name=\"XGBoost\")\n",
    "\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M5q5IroO2B1y",
   "metadata": {
    "id": "M5q5IroO2B1y"
   },
   "source": [
    "Interpretation Question:\n",
    "How should we interpret the comparative ROC curves of Logistic Regression and XGBoost, and what does this reveal about model robustness?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cM8ncPpIkq-s",
   "metadata": {
    "id": "cM8ncPpIkq-s"
   },
   "source": [
    "#Step 8 – SHAP Explainability for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N8rYUXaCp5VA",
   "metadata": {
    "id": "N8rYUXaCp5VA"
   },
   "outputs": [],
   "source": [
    "sample_X = X_test.sample(100, random_state=30)   # FILL IN ALL BLANKS\n",
    "explainer = shap.Explainer(lambda x: xgb_model.predict_proba(x)[:,1], sample_X, algorithm=\"auto\")\n",
    "shap_values = explainer(sample_X)\n",
    "shap.summary_plot(shap_values.values, sample_X, feature_names=sample_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xOpv0pB42Kho",
   "metadata": {
    "id": "xOpv0pB42Kho"
   },
   "source": [
    "Interpretation Question:\n",
    "How do SHAP values enhance the interpretability of complex models like XGBoost, and why might they be preferred over traditional feature importance scores?\n",
    "\n",
    "* SHAP may be preferred over traditional feature importance scores because SHAP values have local and global interpretability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nY-pyOxerLLY",
   "metadata": {
    "id": "nY-pyOxerLLY"
   },
   "source": [
    "#Step 9 – Feature Importance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuNK7pxNsNIQ",
   "metadata": {
    "id": "fuNK7pxNsNIQ"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Feature Importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "coeffs = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coefficient\": log_model.coef_[0]   ### FILL IN ALL BLANKS\n",
    "}).sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "top_features = pd.concat([coeffs.head(15), coeffs.tail(15)])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Coefficient\", y=\"Feature\", data=top_features, palette=\"coolwarm\")\n",
    "plt.title(\"Top & Bottom 15 Feature Importances – Logistic Regression\", fontsize=14)\n",
    "plt.xlabel(\"Coefficient Value\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8MZjQZ8o4eVY",
   "metadata": {
    "id": "8MZjQZ8o4eVY"
   },
   "source": [
    "# Reflection and Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_xRUjxfC4gGT",
   "metadata": {
    "id": "_xRUjxfC4gGT"
   },
   "source": [
    "Question 1:\n",
    "Can prioritizing model accuracy compromise ethical responsibility in healthcare AI systems?\n",
    "\n",
    "\n",
    "Question 2:\n",
    "How could demographic imbalance in the dataset (ex: age or sex distribution) influence fairness in stroke prediction models?\n",
    "\n",
    "\n",
    "Question 3:\n",
    "How can explainability tools like SHAP support ethical compliance and model validation in sensitive domains such as healthcare?\n",
    "\n",
    "\n",
    "Question 4:\n",
    "How does handling missing data intersect with privacy and model transparency principles?\n",
    "\n",
    "\n",
    "Question 5:\n",
    "What responsibilities do data scientists bear when deploying predictive models in healthcare decision-making pipelines?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
