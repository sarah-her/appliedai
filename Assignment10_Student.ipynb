{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88dbaf3",
   "metadata": {
    "id": "b88dbaf3"
   },
   "source": [
    "# Assignment 10 – Applied Component\n",
    "Topic: Model Explainability, Fairness, and Interpretability in Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fac144",
   "metadata": {
    "id": "f7fac144"
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Theoretical Background\n",
    "\n",
    "### 1.1 Logistic Regression\n",
    "\n",
    "Logistic Regression is used for binary classification tasks and models the probability of a given input belonging to a specific class. It uses the logistic (sigmoid) function to map any real-valued number into a range between 0 and 1:\n",
    "\n",
    "$$\n",
    "\\displaystyle\n",
    "P(y = 1 \\mid X) \\;=\\; \\frac{1}{\\,1 + e^{-(\\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\cdots + \\beta_{n}X_{n})}\\,}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Equivalently, in compact summation form:\n",
    "\n",
    "\n",
    "$$\n",
    "\\displaystyle\n",
    "P(y = 1 \\mid X) \\;=\\; \\frac{1}{\\,1 + e^{-(\\beta_{0} + \\sum_{j=1}^{n} \\beta_{j} X_{j})}\\,}\n",
    "$$\n",
    "\n",
    "The model is trained using **maximum likelihood estimation (MLE)**, which finds the parameters (\\(\\beta\\)) that maximize the probability of observing the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452c5c54",
   "metadata": {
    "id": "452c5c54"
   },
   "source": [
    "### 1.2 XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "XGBoost is an ensemble learning algorithm based on gradient boosting. It builds trees sequentially, where each tree corrects the errors of the previous one.\n",
    "\n",
    "The objective function combines a loss term and a regularization term:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ff446",
   "metadata": {
    "id": "f24ff446"
   },
   "source": [
    "$$\n",
    "\\displaystyle\n",
    "\\mathrm{Obj} \\;=\\; \\sum_{i} \\ell\\!\\left(y_i, \\hat{y}_i\\right) \\;+\\; \\sum_{k} \\Omega\\!\\left(f_k\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb34d7",
   "metadata": {
    "id": "9cdb34d7"
   },
   "source": [
    "with regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70529c",
   "metadata": {
    "id": "ef70529c"
   },
   "source": [
    "$$\n",
    "\\displaystyle\n",
    "\\Omega\\!\\left(f_k\\right) \\;=\\; \\gamma\\,T \\;+\\; \\tfrac{1}{2}\\,\\lambda\\,\\lVert w \\rVert^{2}\n",
    "$$\n",
    "\n",
    "Here, \\(T\\) is the number of leaves and \\(w\\) are the leaf weights. Regularization helps prevent overfitting. This makes XGBoost powerful yet regularized to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5b8b6",
   "metadata": {
    "id": "ccd5b8b6"
   },
   "source": [
    "### 1.3 Explainability with SHAP\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) assigns each feature an importance value for a particular prediction. It is based on cooperative game theory, where feature contributions are computed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0108c558",
   "metadata": {
    "id": "0108c558"
   },
   "source": [
    "$$\n",
    "\\displaystyle\n",
    "\\phi_i \\;=\\; \\sum_{S \\subseteq F \\setminus \\{i\\}}\\; \\frac{|S|!\\,\\left(|F|-|S|-1\\right)!}{|F|!}\\;\\Big[\\, f(S \\cup \\{i\\}) \\;-\\; f(S) \\,\\Big]\n",
    "$$\n",
    "\n",
    "This helps in explaining how each input variable influences the model’s output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NoKouMUXgmlT",
   "metadata": {
    "id": "NoKouMUXgmlT"
   },
   "source": [
    "# Step 1 – Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "wX_Hpv1Sgpct",
   "metadata": {
    "id": "wX_Hpv1Sgpct"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Snfcu-P8gvQe",
   "metadata": {
    "id": "Snfcu-P8gvQe"
   },
   "source": [
    "#Step 2 – Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8grW2keEgw_-",
   "metadata": {
    "id": "8grW2keEgw_-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# Detect the correct file encoding\n",
    "with open(\"____\", 'rb') as f:                 # FILL IN ALL BLANKS\n",
    "    result = chardet.detect(f.read(____))\n",
    "\n",
    "# Load dataset safely with detected encoding\n",
    "df = pd.read_csv(\"____\", encoding=result['____'], low_memory=False)\n",
    "\n",
    "print(\"File loaded successfully!\")\n",
    "print(\"Detected encoding:\", result['____'])\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "print(\"\\nPreview of data:\")\n",
    "display(df.____())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QQUZAuXGimj_",
   "metadata": {
    "id": "QQUZAuXGimj_"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\\n\", df.____().sum())    # FILL IN ALL BLANKS\n",
    "\n",
    "# Handle missing data\n",
    "df = df.____(df.____(numeric_only=True))\n",
    "\n",
    "display(df.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ktFMAZpi1Ylh",
   "metadata": {
    "id": "ktFMAZpi1Ylh"
   },
   "source": [
    "Interpretation Question:\n",
    "What would be the possible implications of not handling missing or inconsistent values in this dataset before training predictive models like Logistic Regression or XGBoost?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cB-wLKAMipSW",
   "metadata": {
    "id": "cB-wLKAMipSW"
   },
   "source": [
    "# Step 3 – Feature Correlation and Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lvxn6dTMiosF",
   "metadata": {
    "id": "lvxn6dTMiosF"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Available columns:\\n\", df.columns.tolist())  # FILL IN ALL BLANKS\n",
    "\n",
    "# Identify the likely target column\n",
    "possible_targets = [col for col in df.columns if '____' in col.upper() or '____' in col.upper() or '____' in col.upper()]\n",
    "print(\"\\nPossible target columns:\", possible_targets)\n",
    "\n",
    "df.rename(columns={'____': 'Outcome'}, inplace=True)\n",
    "\n",
    "#visualize\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(x='Outcome', data=df)\n",
    "plt.title(\"Outcome Distribution (0 = No Stroke, 1 = Stroke)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DbtrjUPijnZD",
   "metadata": {
    "id": "DbtrjUPijnZD"
   },
   "source": [
    "#Step 4 – Split Data and Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bOXCMUzCwFOc",
   "metadata": {
    "id": "bOXCMUzCwFOc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Drop obvious leakage columns\n",
    "leakage_cols = [col for col in df.columns if '____' in col or '____' in col or '____' in col]    # FILL IN ALL BLANKS\n",
    "print(\"Dropping leakage columns:\", leakage_cols)\n",
    "X = df.drop(columns=____)\n",
    "y = df['____']\n",
    "\n",
    "# Encode categorical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == '____' or X[col].dtype == '____':\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "X = X.____(0)\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=____, stratify=y, random_state=____\n",
    "\n",
    "# Balance classes with SMOTE\n",
    "sm = SMOTE(random_state=____)\n",
    "X_train_bal, y_train_bal = sm.fit_resample(____, ____ )\n",
    "print(\"After SMOTE:\", y_train_bal.value_counts().to_dict())\n",
    "\n",
    "# Scale for logistic regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.____(____)\n",
    "X_test_scaled = scaler.____(____)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dM_2Avjl1wOR",
   "metadata": {
    "id": "dM_2Avjl1wOR"
   },
   "source": [
    "Interpretation Question:\n",
    "Why is it crucial to encode categorical variables before scaling, and what might occur if categorical features remain unencoded during the training of these models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nqJAwbKPkLfR",
   "metadata": {
    "id": "nqJAwbKPkLfR"
   },
   "source": [
    "#Step 5 – Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6G7BS3YvkQem",
   "metadata": {
    "id": "6G7BS3YvkQem"
   },
   "outputs": [],
   "source": [
    "# Retrain Logistic Regression with SMOTE-balanced data\n",
    "log_model = LogisticRegression(max_iter=____, random_state=____)    # FILL IN ALL BLANKS\n",
    "\n",
    "\n",
    "log_model.____(____, ____ )\n",
    "\n",
    "y_pred_lr = log_model.____(____)\n",
    "\n",
    "print(\"-- Logistic Regression Report --\")\n",
    "print(classification_report(____, ____))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(____, log_model.predict_proba(____)[:,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kQN238D_11eQ",
   "metadata": {
    "id": "kQN238D_11eQ"
   },
   "source": [
    "Interpretation Question:\n",
    "What can we infer about feature relationships from the magnitude and sign of Logistic Regression coefficients, and how does this aid interpretability in a clinical setting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5V6yBW3HkWIM",
   "metadata": {
    "id": "5V6yBW3HkWIM"
   },
   "source": [
    "#Step 6 – XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YQPAHWTTkTXP",
   "metadata": {
    "id": "YQPAHWTTkTXP"
   },
   "outputs": [],
   "source": [
    "# Initialize XGBoost classifier\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=____,         # FILL IN ALL BLANKS\n",
    "    learning_rate=____,\n",
    "    max_depth=____,\n",
    "    subsample=____,\n",
    "    colsample_bytree=____,\n",
    "    random_state=____,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='____'\n",
    ")\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model.____(____, ____ )\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_xgb = xgb_model.____(____)\n",
    "\n",
    "print(\"-- XGBoost Model Report --\")\n",
    "print(classification_report(____, ____))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(____, xgb_model.predict_proba(____)[:,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RiLiw7FQ17ya",
   "metadata": {
    "id": "RiLiw7FQ17ya"
   },
   "source": [
    "Interpretation Question:\n",
    "In comparison to Logistic Regression, how does XGBoost’s gradient-boosting mechanism influence the way it learns patterns from the same dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fzzwa8BZkiyf",
   "metadata": {
    "id": "Fzzwa8BZkiyf"
   },
   "source": [
    "#Step 7 – Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FVFoHlyYkj91",
   "metadata": {
    "id": "FVFoHlyYkj91"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(____, ____))   # FILL IN ALL BLANKS\n",
    "\n",
    "# Plot ROC curves for both models\n",
    "RocCurveDisplay.from_estimator(____, ____, ____, ax=plt.gca(), name=\"Logistic Regression\")\n",
    "RocCurveDisplay.from_estimator(____, ____, ____, ax=plt.gca(), name=\"XGBoost\")\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M5q5IroO2B1y",
   "metadata": {
    "id": "M5q5IroO2B1y"
   },
   "source": [
    "Interpretation Question:\n",
    "How should we interpret the comparative ROC curves of Logistic Regression and XGBoost, and what does this reveal about model robustness?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cM8ncPpIkq-s",
   "metadata": {
    "id": "cM8ncPpIkq-s"
   },
   "source": [
    "#Step 8 – SHAP Explainability for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N8rYUXaCp5VA",
   "metadata": {
    "id": "N8rYUXaCp5VA"
   },
   "outputs": [],
   "source": [
    "sample_X = X_test.sample(____, random_state=____)   # FILL IN ALL BLANKS\n",
    "explainer = shap.Explainer(lambda x: xgb_model.predict_proba(x)[:,____], sample_X, algorithm=\"____\")\n",
    "shap_values = explainer(____)\n",
    "shap.summary_plot(shap_values.values, sample_X, feature_names=____)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xOpv0pB42Kho",
   "metadata": {
    "id": "xOpv0pB42Kho"
   },
   "source": [
    "Interpretation Question:\n",
    "How do SHAP values enhance the interpretability of complex models like XGBoost, and why might they be preferred over traditional feature importance scores?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nY-pyOxerLLY",
   "metadata": {
    "id": "nY-pyOxerLLY"
   },
   "source": [
    "#Step 9 – Feature Importance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuNK7pxNsNIQ",
   "metadata": {
    "id": "fuNK7pxNsNIQ"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Feature Importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "coeffs = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coefficient\": log_model.coef_[____]   ### FILL IN ALL BLANKS\n",
    "}).sort_values(by=\"____\", ascending=____)\n",
    "\n",
    "top_features = pd.concat([coeffs.head(____), coeffs.tail(____)])\n",
    "\n",
    "plt.figure(figsize=(____, ____))\n",
    "sns.barplot(x=\"____\", y=\"____\", data=top_features, palette=\"____\")\n",
    "plt.title(\"Top & Bottom 15 Feature Importances – Logistic Regression\", fontsize=____)\n",
    "plt.xlabel(\"____\", fontsize=____)\n",
    "plt.ylabel(\"____\", fontsize=____)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8MZjQZ8o4eVY",
   "metadata": {
    "id": "8MZjQZ8o4eVY"
   },
   "source": [
    "# Reflection and Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_xRUjxfC4gGT",
   "metadata": {
    "id": "_xRUjxfC4gGT"
   },
   "source": [
    "Question 1:\n",
    "Can prioritizing model accuracy compromise ethical responsibility in healthcare AI systems?\n",
    "\n",
    "\n",
    "Question 2:\n",
    "How could demographic imbalance in the dataset (ex: age or sex distribution) influence fairness in stroke prediction models?\n",
    "\n",
    "\n",
    "Question 3:\n",
    "How can explainability tools like SHAP support ethical compliance and model validation in sensitive domains such as healthcare?\n",
    "\n",
    "\n",
    "Question 4:\n",
    "How does handling missing data intersect with privacy and model transparency principles?\n",
    "\n",
    "\n",
    "Question 5:\n",
    "What responsibilities do data scientists bear when deploying predictive models in healthcare decision-making pipelines?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
