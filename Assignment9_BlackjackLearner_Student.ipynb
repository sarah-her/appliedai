{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Assignment 9 – The Blackjack Learner: Understanding Monte Carlo Reinforcement Learning\n",
        "### Fundamentals of Reinforcement Learning and Monte Carlo Estimation\n",
        "\n",
        "**Topics Covered:** Reinforcement Learning Foundations, Markov Decision Processes (MDPs), Monte Carlo Prediction and Policy Evaluation  \n",
        "**Environment:** A simulated Blackjack game designed to model agent–environment interaction and sequential decision-making under uncertainty.  \n",
        "\n",
        "## Theoretical Concept:\n",
        "\n",
        "Reinforcement Learning (RL) is a computational framework in which an agent learns to make decisions through interaction with an environment.  \n",
        "At each time step \\( t \\), the agent observes a state \\( S_t \\), chooses an action \\( A_t \\), receives a reward \\( R_{t+1} \\), and transitions to a new state \\( S_{t+1} \\).  \n",
        "The goal is to learn a policy \\( \\pi(a|s) \\) that maximizes the expected return\n",
        "\n",
        "\\[\n",
        "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots\n",
        "\\]\n",
        "\n",
        "where \\( \\gamma \\) is the discount factor controlling the importance of future rewards.\n",
        "\n",
        "### Markov Decision Process (MDP):\n",
        "An MDP defines this interaction through the tuple \\( (S, A, P, R, \\gamma) \\), representing states, actions, transition probabilities, rewards, and the discount factor.  \n",
        "The Markov property assumes that the next state depends only on the current state and action, enabling tractable modeling of sequential decisions.\n",
        "\n",
        "### Monte Carlo Prediction:\n",
        "Monte Carlo methods estimate value functions by averaging returns from complete episodes of experience.  \n",
        "For a state \\( s \\), the value estimate \\( V(s) \\) is updated as  \n",
        "\n",
        "\\[\n",
        "V(s) \\leftarrow V(s) + \\alpha [G_t - V(s)]\n",
        "\\]\n",
        "\n",
        "where \\( G_t \\) is the total reward (return) observed after visiting state \\( s \\).  \n",
        "These methods do not require a model of the environment and converge with sufficient sampling.\n",
        "\n",
        "### Assignment Context:\n",
        "In this assignment, a simplified Blackjack environment is used to demonstrate how an agent learns optimal play through experience. By repeatedly playing simulated games, the agent estimates the expected reward of different game states using Monte Carlo averaging. You will visualize learned value patterns, interpret how decisions improve through experience, and discuss the properties of RL algorithms.\n"
      ],
      "metadata": {
        "id": "CjunL7TXeUH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 – Import libraries"
      ],
      "metadata": {
        "id": "aRTzsJAS1NG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "l_Al3cnFeL4f"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 – Define simple helper functions"
      ],
      "metadata": {
        "id": "dK3HW9h31KNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_card():\n",
        "    \"\"\"Draws a random card (1–10).\"\"\"\n",
        "    card = random.randint(1, ____)     ### FILL IN BLANK: max card number\n",
        "    return min(card, ____)             ### FILL IN BLANK: face cards adjustment\n",
        "\n",
        "\n",
        "def hand_value(hand):\n",
        "    \"\"\"Calculates total hand value considering usable ace.\"\"\"\n",
        "    value = ____ (hand)                ### FILL IN BLANK: compute sum\n",
        "    if ____ in hand and value + 10 <= 21:   ### FILL IN BLANK: ace check\n",
        "        return value + 10\n",
        "    return ____                        ### FILL IN BLANK: return total\n",
        "\n",
        "\n",
        "def is_bust(hand):\n",
        "    \"\"\"Returns True if hand value > 21.\"\"\"\n",
        "    return ____ (hand) > 21            ### FILL IN BLANK: helper call\n",
        "\n",
        "\n",
        "# Quick test\n",
        "sample_hand = [____(), ____()]         ### FILL IN BLANK: draw two cards\n",
        "print(\"Sample Hand:\", sample_hand)\n",
        "print(\"Hand Value:\", hand_value(sample_hand))\n",
        "print(\"Is it a bust?\", is_bust(sample_hand))\n"
      ],
      "metadata": {
        "id": "MDPL2Uo_YA7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation Question:**  \n",
        "What is the purpose of the helper functions defined above?"
      ],
      "metadata": {
        "id": "h_D_M5aYel5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 – Simulate one Blackjack episode"
      ],
      "metadata": {
        "id": "3eNddYJV1oY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_blackjack_debug():\n",
        "    player = [____(), ____()]     ### FILL IN BLANK: draw two cards\n",
        "    dealer = [____(), ____()]     ### FILL IN BLANK: draw two cards\n",
        "    episode = []\n",
        "\n",
        "    print(\"\\n--- New Game ---\")\n",
        "    print(f\"Initial Player Hand: {player} (Value = {____(player)})\")   ### FILL IN BLANK\n",
        "    print(f\"Dealer Shows: {dealer[0]}\")\n",
        "\n",
        "    # Define initial state/action\n",
        "    state = (____(player), dealer[0],\n",
        "             1 in player and ____(player) + 10 <= 21)   ### FILL IN BLANKS: compute state\n",
        "    action = \"stick\"\n",
        "\n",
        "    # Player’s turn\n",
        "    while ____(player) < 20:      ### FILL IN BLANK: stopping condition\n",
        "        state = (____(player), dealer[0],\n",
        "                 1 in player and ____(player) + 10 <= 21)\n",
        "\n",
        "        # Simple policy\n",
        "        if ____(player) < 17:     ### FILL IN BLANK: condition for hit\n",
        "            action = \"hit\"\n",
        "        else:\n",
        "            action = \"stick\"\n",
        "\n",
        "        print(f\"\\nPlayer decides to {action.upper()} at value {____(player)}\")\n",
        "\n",
        "        if action == \"hit\":\n",
        "            player.append(____())  ### FILL IN BLANK: draw a card\n",
        "            print(f\"New Player Hand: {player} (Value = {____(player)})\")\n",
        "            if ____(player):       ### FILL IN BLANK: check for bust\n",
        "                print(\"Player busts! Hand value:\", ____(player))\n",
        "                episode.append((state, action, -1))\n",
        "                print(\"Reward: -1 (Loss)\")\n",
        "                return episode\n",
        "        else:\n",
        "            print(\"Player sticks at:\", ____(player))\n",
        "            break\n",
        "\n",
        "    # Dealer’s turn\n",
        "    print(\"\\nDealer's turn begins.\")\n",
        "    while ____(dealer) < 17:      ### FILL IN BLANK\n",
        "        dealer.append(____())     ### FILL IN BLANK\n",
        "        print(f\"Dealer draws. Dealer Hand: {dealer} (Value = {____(dealer)})\")\n",
        "\n",
        "    player_score = ____(player)\n",
        "    dealer_score = ____(dealer)\n",
        "    reward = np.sign(player_score - dealer_score)\n",
        "\n",
        "    print(\"\\nFinal Player Hand:\", player, \"Value:\", player_score)\n",
        "    print(\"Final Dealer Hand:\", dealer, \"Value:\", dealer_score)\n",
        "\n",
        "    if reward == 1:\n",
        "        print(\"Result: Player Wins (+1)\")\n",
        "    elif reward == 0:\n",
        "        print(\"Result: Draw (0)\")\n",
        "    else:\n",
        "        print(\"Result: Dealer Wins (-1)\")\n",
        "\n",
        "    episode.append((state, action, reward))\n",
        "    return episode\n",
        "\n",
        "\n",
        "for i in range(3):\n",
        "    result = play_blackjack_debug()\n",
        "    print(\"Episode data:\", result)\n"
      ],
      "metadata": {
        "id": "Gg9ect1TYE4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation Question:**  \n",
        "Looking at the game outputs above, what do these episode traces show about how the agent interacts with the environment, and how can this information be used for learning?"
      ],
      "metadata": {
        "id": "r4GZTtImfnh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 – Monte Carlo Value Estimation"
      ],
      "metadata": {
        "id": "L9JZeRSo10RY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_value_estimation(num_games=____):     ### FILL IN BLANK: choose total episodes\n",
        "    V = defaultdict(____)   ### FILL IN BLANK: store average returns\n",
        "    N = defaultdict(____)   ### FILL IN BLANK: count visits\n",
        "\n",
        "    print(\"Starting Monte Carlo estimation...\")\n",
        "\n",
        "    for game in range(num_games):\n",
        "        episode = ____()    ### FILL IN BLANK: simulate a game\n",
        "        G = 0\n",
        "        seen = set()\n",
        "\n",
        "        # Loop backward through episode\n",
        "        for state, action, reward in ____ (episode):   ### FILL IN BLANK: iterate in reverse\n",
        "            G += reward\n",
        "            if state not in seen:\n",
        "                seen.add(state)\n",
        "                N[state] += 1\n",
        "                V[state] += (G - V[state]) / N[state]\n",
        "\n",
        "        # Minimal progress output\n",
        "        if game == num_games // ____:   ### FILL IN BLANK: midpoint check\n",
        "            print(\"Halfway through... learning in progress.\")\n",
        "\n",
        "    print(\"Monte Carlo estimation completed.\")\n",
        "    return V\n",
        "\n",
        "\n",
        "# Run estimation\n",
        "V = mc_value_estimation(____)   ### FILL IN BLANK: specify number of games\n",
        "print(\"Number of unique states learned:\", len(V))\n"
      ],
      "metadata": {
        "id": "wKwSXm8dYKDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation Question:**  \n",
        "Why does Monte Carlo estimation use averaging across many episodes?\n",
        "\n"
      ],
      "metadata": {
        "id": "TZi7Ty9JgNM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_states = list(____.items())[:____]    ### FILL IN BLANK: access learned states, choose sample size\n",
        "print(\"\\n--- Sample of Learned State Values ---\")\n",
        "\n",
        "for s, val in ____:                          ### FILL IN BLANK: iterate through sampled states\n",
        "    print(f\"State: {s},  Value: {val:.3f}\")\n"
      ],
      "metadata": {
        "id": "kiLFLDssYNoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation Question:**  \n",
        "What does a positive or negative value indicate for a given state?"
      ],
      "metadata": {
        "id": "Ncz2HVClgU_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 – Visualize Value Function (with and without usable ace)"
      ],
      "metadata": {
        "id": "DddqkbLg2gAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6 – Visualize learned state values\n",
        "\n",
        "usable_ace = np.zeros((____, ____))         ### FILL IN BLANK: set grid dimensions\n",
        "no_usable_ace = np.zeros((____, ____))\n",
        "\n",
        "for (player, dealer, ace) in ____:          ### FILL IN BLANK: iterate through learned states\n",
        "    if 12 <= player <= 21 and 1 <= dealer <= 10:\n",
        "        if ace:\n",
        "            usable_ace[player-12][dealer-1] = ____.get((player, dealer, ace), 0)\n",
        "        else:\n",
        "            no_usable_ace[player-12][dealer-1] = ____.get((player, dealer, ace), 0)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(____, ____))   ### FILL IN BLANK: figure size\n",
        "\n",
        "# With usable ace\n",
        "im1 = axes[0].imshow(usable_ace, origin='lower', cmap='____', vmin=-1, vmax=1)   ### FILL IN BLANK: color map\n",
        "axes[0].set_title('Value with Usable Ace')\n",
        "axes[0].set_xlabel('Dealer Showing')\n",
        "axes[0].set_ylabel('Player Sum')\n",
        "plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "# Without usable ace\n",
        "im2 = axes[1].imshow(no_usable_ace, origin='lower', cmap='____', vmin=-1, vmax=1)\n",
        "axes[1].set_title('Value without Usable Ace')\n",
        "axes[1].set_xlabel('Dealer Showing')\n",
        "axes[1].set_ylabel('Player Sum')\n",
        "plt.colorbar(im2, ax=axes[1])\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zd-6OsGZYR_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xujxbbQT6wPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation Question:**  \n",
        "How does having a usable ace influence the expected return?\n"
      ],
      "metadata": {
        "id": "hQPwUlo3hQ1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reflection and Discussion**\n",
        "\n",
        "**Question 1:**  \n",
        "During training, you observed the progress messages as games were completed. What does the increasing number of games tell you about the stability or convergence of the estimated state values? How would you know if the value function has converged?\n",
        "\n",
        "**Question 2:**  \n",
        "Examine the value heatmaps for usable and non-usable aces. What insights can you draw about the role of flexibility in decision-making, and how does the presence of a usable ace change optimal behavior?\n",
        "\n",
        "**Question 3:**  \n",
        "Review the printed sample of learned state values. Some states have slightly negative values even after many episodes. What factors could cause persistent underestimation or overestimation in Monte Carlo value estimation?\n",
        "\n",
        "**Question 4:**  \n",
        "Consider the random action policy used during game simulation. If the agent instead followed a more informed policy (e.g., hit below 17, stick otherwise), how might that affect both the learned values and the learning efficiency?\n",
        "\n",
        "**Question 5:**  \n",
        "Reflect on the overall learning pattern seen in the heatmaps and sample outputs. What does this tell you about the relationship between experiential learning and decision quality in reinforcement learning systems?"
      ],
      "metadata": {
        "id": "lG0-4yqojbMj"
      }
    }
  ]
}