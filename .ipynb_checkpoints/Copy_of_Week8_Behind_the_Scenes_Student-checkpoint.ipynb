{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# In Class Assignment – \"Behind the Scenes: Predicting Box Office Success with Random Forest\"\n",
        "\n",
        "\n",
        "**Dataset:** IMDB-Movie-Data.csv  \n",
        "**Goal:** Predict whether a movie is a *Hit* (rating ≥ 7.0) or *Flop* using Random Forest.\n",
        "\n",
        "\n",
        "## Theoretical Concept: Random Forests\n",
        "\n",
        "### Overview\n",
        "A **Random Forest** is an ensemble machine learning algorithm that combines the predictions of multiple **Decision Trees** to improve accuracy and reduce overfitting.  \n",
        "Each tree in the forest is trained on a random subset of the data and features — a process called **bagging (Bootstrap Aggregating)** — which introduces diversity among trees and ensures the overall model generalizes better than any single tree.\n",
        "\n",
        "---\n",
        "\n",
        "### How It Works?\n",
        "1. **Bootstrap Sampling:**  \n",
        "   Each Decision Tree is trained on a randomly sampled subset of the original dataset (with replacement).  \n",
        "   This ensures that each tree sees slightly different data, creating model diversity.\n",
        "\n",
        "2. **Feature Randomness:**  \n",
        "   At each node, a random subset of features is chosen to determine the best split instead of considering all features.  \n",
        "   This prevents dominant predictors (like “Votes”) from controlling every split, improving generalization.\n",
        "\n",
        "3. **Voting Mechanism:**  \n",
        "   In classification tasks, each tree makes an independent prediction.  \n",
        "   The final output of the Random Forest is decided by **majority voting** among all trees.\n",
        "\n",
        "---\n",
        "\n",
        "### Mathematical Intuition:\n",
        "If \\( T_1, T_2, ..., T_n \\) are the predictions from individual trees, then for classification:\n",
        "\n",
        "\\[\n",
        "\\hat{y} = \\text{mode}(T_1, T_2, ..., T_n)\n",
        "\\]\n",
        "\n",
        "For regression problems, the average prediction is used:\n",
        "\n",
        "\\[\n",
        "\\hat{y} = \\frac{1}{n} \\sum_{i=1}^n T_i\n",
        "\\]\n",
        "\n",
        "This aggregation reduces variance and stabilizes the prediction performance.\n",
        "\n",
        "---\n",
        "\n",
        "### Advantages:\n",
        "- **Reduces Overfitting:** Combining many trees reduces the noise and variance of individual trees.  \n",
        "- **Improves Accuracy:** Random Forests often outperform single models by leveraging ensemble learning.  \n",
        "- **Feature Importance:** They naturally provide insights into which features are most influential.  \n",
        "- **Handles Nonlinear Relationships:** Can model complex, nonlinear patterns without heavy preprocessing.\n",
        "\n",
        "---\n",
        "\n",
        "### Bias-Variance Trade-off:\n",
        "A single Decision Tree tends to have **low bias but high variance** — it can overfit easily.  \n",
        "Random Forests lower the variance by averaging multiple trees while keeping bias relatively low.  \n",
        "This balance leads to stronger performance on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### Application in This Assignment\n",
        "In this assignment, Random Forests are used to predict whether a movie is a **Hit** (IMDb rating ≥ 7.0) or **Flop** based on features such as:\n",
        "- **Votes:** audience engagement  \n",
        "- **Revenue (Millions):** box office performance  \n",
        "- **Metascore:** critic evaluation  \n",
        "- **Runtime (Minutes):** viewer accessibility\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Key Terms\n",
        "| Concept | Description |\n",
        "|----------|-------------|\n",
        "| **Bootstrap Sampling** | Randomly drawing data samples with replacement for each tree |\n",
        "| **Feature Bagging** | Random selection of features for each split |\n",
        "| **Majority Voting** | Aggregating predictions from multiple trees |\n",
        "| **Overfitting** | When the model learns noise instead of true patterns |\n",
        "| **Feature Importance** | Quantitative measure of how much each variable contributes to prediction |\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "Random Forests combine simplicity, interpretability, and power, making them ideal for real-world prediction tasks like analyzing movie success.  \n",
        "They reflect how multiple weak learners, when combined, can form a **strong, stable model** — much like how collective decisions often outperform individual judgments.\n",
        "\n"
      ],
      "metadata": {
        "id": "lnsZXa-JrYax"
      },
      "id": "lnsZXa-JrYax"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 – Load Data"
      ],
      "metadata": {
        "id": "26nrEJGHsbVo"
      },
      "id": "26nrEJGHsbVo"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('____')   ### FILL IN BLANK\n",
        "print(\"Dataset shape:\", ____)   ### FILL IN BLANK\n",
        "print(\"\\nColumns:\\n\", ____)   ### FILL IN BLANK\n",
        "print(\"\\nMissing values:\\n\", ____)   ### FILL IN BLANK\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "B8WVhupUYbkB"
      },
      "id": "B8WVhupUYbkB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**  \n",
        "Which features from this dataset do you think most strongly determine whether a movie is a hit or a flop?\n"
      ],
      "metadata": {
        "id": "tkofL67DMUwA"
      },
      "id": "tkofL67DMUwA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. Data Cleaning and Feature Engineering"
      ],
      "metadata": {
        "id": "Qc1emJpnsZAq"
      },
      "id": "Qc1emJpnsZAq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing key numerical values\n",
        "df = df.dropna(subset=['____','____','____','____'])   ### FILL IN BLANK\n",
        "\n",
        "# Create a binary target: 1 = Hit (Rating ≥ 7), 0 = Flop\n",
        "df['____'] = (df['____'] >= ____).astype(int).  ### FILL IN BLANK\n",
        "\n",
        "# Select relevant numeric features\n",
        "features = ['____','____','____','____']   ### FILL IN BLANK\n",
        "X = df[features]\n",
        "y = df['____']\n",
        "\n",
        "print(\"Features:\", features)\n",
        "print(\"Target distribution:\\n\", y.value_counts(normalize=True))\n"
      ],
      "metadata": {
        "id": "lRDB95xlxLMK",
        "collapsed": true
      },
      "id": "lRDB95xlxLMK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**  \n",
        "Why might features like votes and revenue be more predictive of success compared to runtime or metascore?\n"
      ],
      "metadata": {
        "id": "df0UHYYUQCFi"
      },
      "id": "df0UHYYUQCFi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3. Train/Test Split"
      ],
      "metadata": {
        "id": "BKcRzK_QxTKp"
      },
      "id": "BKcRzK_QxTKp"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    ____, ____, test_size=____, random_state=____, stratify=____   ### FILL IN BLANKS\n",
        ")\n",
        "print(\"Training samples:\", X_train.shape[0], \" Testing samples:\", X_test.shape[0])\n"
      ],
      "metadata": {
        "id": "ILuz6fsixWVt"
      },
      "id": "ILuz6fsixWVt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**  \n",
        "Why is stratified sampling useful when the dataset has a mix of hits and flops?"
      ],
      "metadata": {
        "id": "F6Zh-1hqZRa-"
      },
      "id": "F6Zh-1hqZRa-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4. Train Random Forest Classifier"
      ],
      "metadata": {
        "id": "IdScoUdjxe1W"
      },
      "id": "IdScoUdjxe1W"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=____, max_depth=____, random_state=____,\n",
        "    class_weight='____', n_jobs=____   ### FILL IN BLANKS\n",
        ")\n",
        "rf.fit(____, ____)   ### FILL IN BLANK\n",
        "\n",
        "y_pred = rf.predict(____)   ### FILL IN BLANK\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(____, ____))   ### FILL IN BLANK\n"
      ],
      "metadata": {
        "id": "8Fzz53Eixils"
      },
      "id": "8Fzz53Eixils",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**  \n",
        "How does using many trees help Random Forests reduce overfitting compared to a single Decision Tree?\n"
      ],
      "metadata": {
        "id": "ICEm9PYfMr5U"
      },
      "id": "ICEm9PYfMr5U"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5. Evaluate Model Performance"
      ],
      "metadata": {
        "id": "YFE4hsVzQnhB"
      },
      "id": "YFE4hsVzQnhB"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(____, ____))   ### FILL IN BLANK\n",
        "print(\"\\nClassification Report:\\n\", classification_report(____, ____))   ### FILL IN BLANK\n",
        "\n"
      ],
      "metadata": {
        "id": "0HiOCyjuxwbw"
      },
      "id": "0HiOCyjuxwbw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**  \n",
        "Which metric (precision, recall, or F1) gives you the best insight into the model’s reliability for predicting hit movies?\n"
      ],
      "metadata": {
        "id": "-8rXepebM1UH"
      },
      "id": "-8rXepebM1UH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6. Feature Importance Visualization"
      ],
      "metadata": {
        "id": "exwNAFfOx1-Q"
      },
      "id": "exwNAFfOx1-Q"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "importances = pd.Series(____.feature_importances_, index=____).sort_values(ascending=False)   ### FILL IN BLANK\n",
        "importances.plot(kind='barh', color='lightgreen', title='Feature Importance in Random Forest')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Uk1zhi0xx6ag"
      },
      "id": "Uk1zhi0xx6ag",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**  \n",
        "Which feature was most influential, and does this align with your intuition about what makes a movie successful?\n"
      ],
      "metadata": {
        "id": "eEzN0z6ZQ4I1"
      },
      "id": "eEzN0z6ZQ4I1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7. Hyperparameter Experimentation (Group Task)"
      ],
      "metadata": {
        "id": "ljV7voQNQycU"
      },
      "id": "ljV7voQNQycU"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for n in [____, ____, ____, ____]:   ### FILL IN BLANK\n",
        "    model = RandomForestClassifier(n_estimators=n, random_state=____)   ### FILL IN BLANK\n",
        "    model.fit(____, ____)   ### FILL IN BLANK\n",
        "    preds = model.predict(____)   ### FILL IN BLANK\n",
        "    acc = accuracy_score(____, ____)   ### FILL IN BLANK\n",
        "    print(f\"n_estimators = {n}: Accuracy = {acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "kpo5FBh3yZ5F"
      },
      "id": "kpo5FBh3yZ5F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**  \n",
        "As you increase the number of trees, what trend do you observe in model accuracy and stability?\n"
      ],
      "metadata": {
        "id": "zn0BmrkrQ92R"
      },
      "id": "zn0BmrkrQ92R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8. Visualize ROC Curve"
      ],
      "metadata": {
        "id": "khxPjt7QRCeF"
      },
      "id": "khxPjt7QRCeF"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "y_proba = ____.predict_proba(____)[:, 1]   ### FILL IN BLANK\n",
        "fpr, tpr, _ = roc_curve(____, ____)   ### FILL IN BLANK\n",
        "auc = roc_auc_score(____, ____)   ### FILL IN BLANK\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, label=f\"Random Forest (AUC = {auc:.3f})\")\n",
        "plt.plot([0,1],[0,1],'--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve – Random Forest Classifier\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ylEiW2R9yAdm"
      },
      "id": "ylEiW2R9yAdm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**  \n",
        "What does the ROC-AUC score tell you about the overall quality of your Random Forest model?\n"
      ],
      "metadata": {
        "id": "NkJiSxzrNIad"
      },
      "id": "NkJiSxzrNIad"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Reflection and Discussion"
      ],
      "metadata": {
        "id": "6TmOvFRYzmWW"
      },
      "id": "6TmOvFRYzmWW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. How does combining multiple Decision Trees improve predictive performance?  \n",
        "2. Which features were most influential in predicting movie success?  \n",
        "3. How does increasing the number of trees affect bias and variance?  \n",
        "4. In what ways could this model be improved (e.g., adding categorical features, tuning hyperparameters)?\n"
      ],
      "metadata": {
        "id": "uxw2Rn1cy4oD"
      },
      "id": "uxw2Rn1cy4oD"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}