{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61191c0b",
   "metadata": {},
   "source": [
    "### Amazon Reviews Analysis with Unsupervised Learning\n",
    "\n",
    "In this notebook, we explore how unsupervised learning methods can be applied to real-world text data,\n",
    "specifically Amazon product reviews. Companies rely heavily on customer feedback to improve products,\n",
    "understand user experience, and identify trends. However, reviews are unstructured free-text data, \n",
    "which makes it challenging to analyze at scale without appropriate methods.\n",
    "\n",
    "We will implement multiple approaches for text representation, clustering, and topic modeling, and \n",
    "evaluate their effectiveness. Our goals are to:\n",
    "\n",
    "- Transform reviews into numerical representations (Bag-of-Words, TF-IDF, Word2Vec)\n",
    "- Group reviews into clusters based on similarity\n",
    "- Extract latent themes (topics) from reviews using topic modeling\n",
    "- Reflect on potential biases and propose improvements\n",
    "\n",
    "These techniques are widely used in industry, including for recommender systems, \n",
    "customer support analysis, and product quality monitoring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed4a342",
   "metadata": {},
   "source": [
    "### Step 1: Load Amazon Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a9fa3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>variation</th>\n",
       "      <th>verified_reviews</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-31 00:00:00</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Love my Echo!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-31 00:00:00</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Loved it!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-07-31 00:00:00</td>\n",
       "      <td>Walnut Finish</td>\n",
       "      <td>\"Sometimes while playing a game, you can answe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-31 00:00:00</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>\"I have had a lot of fun with this thing. My 4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-31 00:00:00</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Music</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                 date         variation  \\\n",
       "0       5  2018-07-31 00:00:00  Charcoal Fabric    \n",
       "1       5  2018-07-31 00:00:00  Charcoal Fabric    \n",
       "2       4  2018-07-31 00:00:00    Walnut Finish    \n",
       "3       5  2018-07-31 00:00:00  Charcoal Fabric    \n",
       "4       5  2018-07-31 00:00:00  Charcoal Fabric    \n",
       "\n",
       "                                    verified_reviews  feedback  \n",
       "0                                      Love my Echo!         1  \n",
       "1                                          Loved it!         1  \n",
       "2  \"Sometimes while playing a game, you can answe...         1  \n",
       "3  \"I have had a lot of fun with this thing. My 4...         1  \n",
       "4                                              Music         1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "# Ensure the dataset has at least one column with review text.\n",
    "\n",
    "amazon = pd.read_csv(r'C:\\Users\\sarah\\Desktop\\BC\\Fall 2025\\MESA8414 Applied AI and Machine Learning\\Assignment\\Week 6 assignment\\amazon-alexa.csv')   ###FILL IN BLANK: dataset path\n",
    "amazon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6075213",
   "metadata": {},
   "source": [
    "### Step 2: Text Representation with Bag-of-Words and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea706e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW shape: (3150, 4044)\n",
      "TF-IDF shape: (3150, 4044)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "text_col = \"verified_reviews\"\n",
    "\n",
    "def normalize_text(x):\n",
    "    if isinstance(x, list):\n",
    "        return \" \".join(map(str, x))\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return str(x)\n",
    "\n",
    "amazon[text_col] = amazon[text_col].apply(normalize_text)\n",
    "amazon_clean = amazon[amazon[text_col].str.strip().ne(\"\")].copy()\n",
    "\n",
    "# Bag-of-Words\n",
    "vectorizer = CountVectorizer(max_features=5000)   ###FILL IN BLANK\n",
    "X_bow = vectorizer.fit_transform(amazon['verified_reviews'])   ###FILL IN BLANK\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)   ###FILL IN BLANK\n",
    "X_tfidf = tfidf.fit_transform(amazon['verified_reviews'])   ###FILL IN BLANK\n",
    "\n",
    "print('BoW shape:', X_bow.shape)\n",
    "print('TF-IDF shape:', X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c22be5",
   "metadata": {},
   "source": [
    "Using the above analysis, elaborate on:\n",
    "- What kind of information BoW captures and what it misses: Bow shape of 3150, 4044 means that 4044 unique words are taken from 3150 documents. However, because BoW only counts the frequency of words present, it misses the semantic relationships between words. It also does not take into account the ordering of words or compound words like New York.\n",
    "  \n",
    "- Why TF-IDF can sometimes be more effective: TF-IDF can be more effective when distinguishing documents is an important task. It gives low weights to high-frequency words like \"the\" \"a\" or \"this\" and gives more weight to low-frequency words that may carry actual information and meaning.\n",
    "  \n",
    "- How dimensionality (features) impacts representation: Dimensionality can provide nuanced and detailed analysis, but high dimensionality can be computationally very demanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a1504",
   "metadata": {},
   "source": [
    "### Step 3: Word Embeddings with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b650f4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('speaker', 0.9995638132095337),\n",
       " ('The', 0.9995536804199219),\n",
       " ('great', 0.9994460940361023),\n",
       " ('is', 0.9994190335273743),\n",
       " ('This', 0.9993958473205566),\n",
       " ('a', 0.9993539452552795),\n",
       " ('great.', 0.9993239045143127),\n",
       " ('sound', 0.9993112683296204),\n",
       " ('sounds', 0.9992988109588623),\n",
       " ('well', 0.9992895126342773)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenization\n",
    "amazon['tokens'] = amazon['verified_reviews'].astype(str).apply(lambda x: x.split())\n",
    "\n",
    "\n",
    "# Train Word2Vec\n",
    "model = Word2Vec(sentences=amazon['tokens'], vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Similar words\n",
    "model.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027168a4",
   "metadata": {},
   "source": [
    "Using the above analysis, elaborate on:\n",
    "- Do the similar words make intuitive sense? Some of the similar words make intuitive sense, but not others. For example, words such as \"speaker\", \"sounds\", and \"device\" could be said to carry semantic similarity. However, \"The\", \"is\", \"This\", and \"a\" do not carry meaning information in and of themselves. Although these words may have appeared frequently together with words semantically similar to speaker, they don't provide useful insights.\n",
    "  \n",
    "- What advantages embeddings provide compared to BoW/TF-IDF: Embeddings can capture contextual similarity. This is a huge advantage over Bow/TF-IDF that only analyzed frequencies.\n",
    "\n",
    "  \n",
    "- Situations where embeddings might fail: There may be a situation where embeddings fail. Words that were not seen during training get no embedding. Therefore, if the training dataset is small or if the trained algorithm is applied to documents that introduce new vocabulary in another area, embeddings may fail. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf1e970",
   "metadata": {},
   "source": [
    "### Step 4: Clustering Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "600e88d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KMeans Results ===\n",
      "Cluster labels: [3 1 9 ... 9 9 7]\n",
      "Cluster counts: [ 33 433 126 260 510 164  94 720 113 697]\n",
      "\n",
      "=== Agglomerative Results ===\n",
      "Cluster labels: [0 0 5 ... 5 5 2]\n",
      "Cluster counts: [1643  219  244   54   28  768   35   38   24   97]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "import numpy as np\n",
    "\n",
    "# Choose representation\n",
    "X = X_tfidf.toarray()   ###FILL IN BLANK if using another representation\n",
    "\n",
    "# KMeans clustering\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)   ###FILL IN BLANK\n",
    "kmeans_labels = kmeans.fit_predict(X)\n",
    "\n",
    "print(\"=== KMeans Results ===\")\n",
    "print(\"Cluster labels:\", kmeans_labels)\n",
    "print(\"Cluster counts:\", np.bincount(kmeans_labels))\n",
    "print()\n",
    "\n",
    "# Hierarchical clustering\n",
    "agg = AgglomerativeClustering(n_clusters=10)   ###FILL IN BLANK\n",
    "agg_labels = agg.fit_predict(X)\n",
    "\n",
    "print(\"=== Agglomerative Results ===\")\n",
    "print(\"Cluster labels:\", agg_labels)\n",
    "print(\"Cluster counts:\", np.bincount(agg_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e4f8120-0787-481d-b4b8-05f4bc263a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KMeans: Top Terms per Cluster ===\n",
      "\n",
      "Cluster 0 (n=33):\n",
      "['expected', 'as', 'more', 'does', 'and', 'than', 'what', 'works', 'arrived', 'everything']\n",
      "\n",
      "Cluster 1 (n=433):\n",
      "['it', 'was', 'my', 'to', 'and', 'for', 'the', 'but', 'works', 'not']\n",
      "\n",
      "Cluster 2 (n=126):\n",
      "['like', 'it', 'new', 'just', 'really', 'works', 'other', 'charm', 'the', 'far']\n",
      "\n",
      "Cluster 3 (n=260):\n",
      "['love', 'it', 'my', 'echo', 'this', 'alexa', 'everything', 'so', 'the', 'absolutely']\n",
      "\n",
      "Cluster 4 (n=510):\n",
      "['the', 'and', 'we', 'echo', 'is', 'to', 'of', 'with', 'for', 'in']\n",
      "\n",
      "Cluster 5 (n=164):\n",
      "['easy', 'set', 'up', 'to', 'use', 'and', 'very', 'setup', 'it', 'was']\n",
      "\n",
      "Cluster 6 (n=94):\n",
      "['love', 'it', 'great', 'still', 'kids', 'amazing', 'said', 'enough', 'say', 'well']\n",
      "\n",
      "Cluster 7 (n=720):\n",
      "['good', 'works', 'great', 'very', 'sound', 'and', 'as', 'nice', 'product', 'awesome']\n",
      "\n",
      "Cluster 8 (n=113):\n",
      "['great', 'product', 'works', 'sound', 'it', 'price', 'use', 'and', 'device', 'buy']\n",
      "\n",
      "Cluster 9 (n=697):\n",
      "['to', 'the', 'it', 'and', 'my', 'is', 'you', 'have', 'for', 'that']\n",
      "\n",
      "=== Agglomerative: Top Terms per Cluster ===\n",
      "\n",
      "Cluster 0 (n=1643):\n",
      "['it', 'the', 'love', 'to', 'and', 'my', 'for', 'echo', 'great', 'is']\n",
      "\n",
      "Cluster 1 (n=219):\n",
      "['easy', 'to', 'it', 'use', 'and', 'still', 'learning', 'love', 'set', 'up']\n",
      "\n",
      "Cluster 2 (n=244):\n",
      "['great', 'very', 'good', 'product', 'sound', 'amazing', 'nice', 'quality', 'and', 'speaker']\n",
      "\n",
      "Cluster 3 (n=54):\n",
      "['everything', 'expected', 'does', 'it', 'as', 'more', 'what', 'love', 'about', 'arrived']\n",
      "\n",
      "Cluster 4 (n=28):\n",
      "['like', 'it', 'really', 'charm', 'other', 'works', 'just', 'one', 'all', 'own']\n",
      "\n",
      "Cluster 5 (n=768):\n",
      "['the', 'to', 'it', 'and', 'my', 'is', 'we', 'echo', 'for', 'have']\n",
      "\n",
      "Cluster 6 (n=35):\n",
      "['great', 'product', 'love', 'it', 'this', 'feee', 'feeds', 'fairness', 'fairly', 'fair']\n",
      "\n",
      "Cluster 7 (n=38):\n",
      "['set', 'easy', 'up', 'to', 'and', 'use', 'was', 'great', 'works', 'sound']\n",
      "\n",
      "Cluster 8 (n=24):\n",
      "['works', 'great', 'it', 'fails', 'facts', 'fail', 'failed', 'failing', 'útil', 'factor']\n",
      "\n",
      "Cluster 9 (n=97):\n",
      "['love', 'it', 'this', 'great', 'kids', 'thing', 'device', 'works', 'and', 'to']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Top terms per KMeans cluster \n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "def top_terms_for_centroid(centroid, n=10):\n",
    "    idx = np.argsort(centroid)[::-1][:n]\n",
    "    return list(zip(feature_names[idx], centroid[idx]))\n",
    "\n",
    "print(\"=== KMeans: Top Terms per Cluster ===\")\n",
    "for c in range(kmeans.n_clusters):\n",
    "    print(f\"\\nCluster {c} (n={np.sum(kmeans_labels==c)}):\")\n",
    "    print([t for t,_ in top_terms_for_centroid(kmeans.cluster_centers_[c], n=10)])\n",
    "\n",
    "# 2) Top terms per Agglomerative cluster \n",
    "X_dense = X\n",
    "print(\"\\n=== Agglomerative: Top Terms per Cluster ===\")\n",
    "for c in range(agg.n_clusters):\n",
    "    mask = (agg_labels == c)\n",
    "    centroid = X_dense[mask].mean(axis=0)\n",
    "    centroid = np.asarray(centroid).ravel()\n",
    "    print(f\"\\nCluster {c} (n={mask.sum()}):\")\n",
    "    print([t for t,_ in top_terms_for_centroid(centroid, n=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed1e18b",
   "metadata": {},
   "source": [
    "Using the above analysis, elaborate on:\n",
    "- What kinds of reviews are grouped together? To see what kinds of reviews are grouped together, I had to look at the top terms that appear in each cluster. For some of the clusters, it looks like it did a fairly good job of putting semantically similar words into the same cluster. For example, K-means cluster # 7 contains words such as 'good', 'works', 'great', 'nice', and 'awesome.' However, other clusters like Cluster 4 in the K-means clustering contain mostly the words like 'the', 'and', 'we', and 'and' that do not carry meaning of their own. The same is true for agglomerative clustering. - \n",
    "  \n",
    "- Differences between KMeans and Hierarchical clustering results; With K-means clustering, I see that there are several medium-to-large clusters. However, with hierarchical clustering, I see one very large cluster and several relatively smaller ones.\n",
    "  \n",
    "- How cluster interpretability depends on chosen representation: Cluster interpretability depends on the chosen representation of clusters. Since with Bow/TF-IDF, the words that most frequently appeared are the top terms, you can clearly see the topic labels for each cluster. With word embeddings, although the context is preserved, it may be harder to interpret clusters directly by just looking at the raw vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ebd637",
   "metadata": {},
   "source": [
    "### Step 5: Topic Modeling (LDA and BERTopic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f4ee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 ['far', 'alexa', 'you', 'like', 'so', 'and', 'the', 'love', 'everything', 'it']\n",
      "Topic 1 ['loves', 'really', 'this', 'spot', 'great', 'like', 'my', 'echo', 'it', 'love']\n",
      "Topic 2 ['are', 'echo', 'enjoy', 'new', 'and', 'family', 'the', 'to', 'we', 'my']\n",
      "Topic 3 ['volume', 'issue', 'champ', 'very', 'excited', 'clear', 'friendly', 'user', 'low', 'fantastic']\n",
      "Topic 4 ['it', 'great', 'stick', 'works', 'excellent', 'very', 'sound', 'speaker', 'perfect', 'good']\n",
      "Topic 5 ['have', 'echo', 'love', 'is', 'for', 'my', 'and', 'it', 'to', 'the']\n",
      "Topic 6 ['second', 'purchased', 'was', 'third', 'problems', 'gift', 'working', 'fast', 'no', 'ease']\n",
      "Topic 7 ['now', 'beyond', 'twice', 'listens', 'device', 'friend', 'rocks', 'alexa', 'phenomenal', 'awesome']\n",
      "Topic 8 ['my', 'it', 'of', 'great', 'is', 'quality', 'fun', 'sound', 'amazing', 'the']\n",
      "Topic 9 ['very', 'and', 'use', 'to', 'up', 'set', 'works', 'product', 'easy', 'great']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# LDA\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42)  \n",
    "lda.fit(X_tfidf)\n",
    "\n",
    "# Display top words per topic\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {idx}\", [tfidf.get_feature_names_out()[i] for i in topic.argsort()[-10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a489dcbc-088c-48db-a3d7-68b11aea9478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertopic in c:\\users\\sarah\\anaconda3\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from bertopic) (0.8.40)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from bertopic) (0.5.9.post2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from bertopic) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from bertopic) (2.2.2)\n",
      "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from bertopic) (5.24.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from bertopic) (1.7.2)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from bertopic) (5.1.1)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from bertopic) (4.66.5)\n",
      "Requirement already satisfied: llvmlite>0.36.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from bertopic) (0.43.0)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (24.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0->bertopic) (3.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.56.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (2.8.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.35.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.6)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (75.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.6.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e6943f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876630dd51a34c52a658802f927ed556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6821a2f0724005a6e87768ac09d035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826f46e0bc2f4719a9a02579e24e5639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4297f8b18343466ba6d543b2f2f9dbe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c528b48bd6b84fbc8fb00a0ec6da6f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6133918992d048d3bda82fa0785d0342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3a2d1d1e874e4cbc919284c461d127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8a7e1cfae24b8ab897fd28a60a4d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51863a43a7843eaa1676eae2cfc1534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a144fb13a31544249905bfafb8f703f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff76774d66945e28893dfe5a24d927a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>879</td>\n",
       "      <td>-1_the_to_and_it</td>\n",
       "      <td>[the, to, and, it, is, my, have, for, not, with]</td>\n",
       "      <td>[\"Much easier to use than the dot. It picks up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>0_stick_fire_tv_cable</td>\n",
       "      <td>[stick, fire, tv, cable, firestick, watch, net...</td>\n",
       "      <td>[I love my fire stick, We all just love the TV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>1_alexa_she_we_or</td>\n",
       "      <td>[alexa, she, we, or, our, her, of, off, that, to]</td>\n",
       "      <td>[\"I purchased this for my mother who is having...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>91</td>\n",
       "      <td>2_music_play_being_listen</td>\n",
       "      <td>[music, play, being, listen, it, all, audioboo...</td>\n",
       "      <td>[I like it alot. I connected it so I can play ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>69</td>\n",
       "      <td>3_clock_alarm_night_at</td>\n",
       "      <td>[clock, alarm, night, at, for, as, this, the, ...</td>\n",
       "      <td>[\"This was given to my 7 year at the time as a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>75</td>\n",
       "      <td>13</td>\n",
       "      <td>75_works_great_fine_awesome</td>\n",
       "      <td>[works, great, fine, awesome, any, other, one,...</td>\n",
       "      <td>[Works great!, Works great!, Works great!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>76_plus_pleasure_addition_echo</td>\n",
       "      <td>[plus, pleasure, addition, echo, facts, leisur...</td>\n",
       "      <td>[Great addition to my Echo Plus!, Love my new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>77</td>\n",
       "      <td>13</td>\n",
       "      <td>77_tv_smart_manufacturerslg_grandsons</td>\n",
       "      <td>[tv, smart, manufacturerslg, grandsons, linksp...</td>\n",
       "      <td>[\"I’m having trouble connecting my tv to it, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>78</td>\n",
       "      <td>12</td>\n",
       "      <td>78_quality_thiswont_expert_sound</td>\n",
       "      <td>[quality, thiswont, expert, sound, massive, ne...</td>\n",
       "      <td>[Love it.  It works great.  Alexa still has so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>79</td>\n",
       "      <td>11</td>\n",
       "      <td>79_far_good_worthless_sad</td>\n",
       "      <td>[far, good, worthless, sad, joke, so, low, pre...</td>\n",
       "      <td>[So far so good, So far so good., So far so good]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                   Name  \\\n",
       "0      -1    879                       -1_the_to_and_it   \n",
       "1       0    124                  0_stick_fire_tv_cable   \n",
       "2       1    112                      1_alexa_she_we_or   \n",
       "3       2     91              2_music_play_being_listen   \n",
       "4       3     69                 3_clock_alarm_night_at   \n",
       "..    ...    ...                                    ...   \n",
       "76     75     13            75_works_great_fine_awesome   \n",
       "77     76     13         76_plus_pleasure_addition_echo   \n",
       "78     77     13  77_tv_smart_manufacturerslg_grandsons   \n",
       "79     78     12       78_quality_thiswont_expert_sound   \n",
       "80     79     11              79_far_good_worthless_sad   \n",
       "\n",
       "                                       Representation  \\\n",
       "0    [the, to, and, it, is, my, have, for, not, with]   \n",
       "1   [stick, fire, tv, cable, firestick, watch, net...   \n",
       "2   [alexa, she, we, or, our, her, of, off, that, to]   \n",
       "3   [music, play, being, listen, it, all, audioboo...   \n",
       "4   [clock, alarm, night, at, for, as, this, the, ...   \n",
       "..                                                ...   \n",
       "76  [works, great, fine, awesome, any, other, one,...   \n",
       "77  [plus, pleasure, addition, echo, facts, leisur...   \n",
       "78  [tv, smart, manufacturerslg, grandsons, linksp...   \n",
       "79  [quality, thiswont, expert, sound, massive, ne...   \n",
       "80  [far, good, worthless, sad, joke, so, low, pre...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [\"Much easier to use than the dot. It picks up...  \n",
       "1   [I love my fire stick, We all just love the TV...  \n",
       "2   [\"I purchased this for my mother who is having...  \n",
       "3   [I like it alot. I connected it so I can play ...  \n",
       "4   [\"This was given to my 7 year at the time as a...  \n",
       "..                                                ...  \n",
       "76         [Works great!, Works great!, Works great!]  \n",
       "77  [Great addition to my Echo Plus!, Love my new ...  \n",
       "78  [\"I’m having trouble connecting my tv to it, b...  \n",
       "79  [Love it.  It works great.  Alexa still has so...  \n",
       "80  [So far so good, So far so good., So far so good]  \n",
       "\n",
       "[81 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERTopic (optional)\n",
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(amazon['verified_reviews'])\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4a384",
   "metadata": {},
   "source": [
    "Using the above analysis, elaborate on:\n",
    "- Which method (LDA vs BERTopic) seems more coherent: the topics from LDA show a lot of stopwords like 'it', 'is', and 'the'. On the other hand, topics from BERTopic feature more words that are semantically coherent. For example, 'clock', 'alarm', night', etc.\n",
    "  \n",
    "- Practical insights companies can gain from extracted topics: I think companies can gain much practical insights from extracted topics, especially the BERTopic. I really liked the representative doc column so you can get more contextualized information for each cluster.\n",
    "\n",
    "  \n",
    "- Challenges of topic modeling with short reviews: I think that with short reviews, there are a small number of informative words and a lot of stopwords such as \"I\" and \"The.\" Also, simple and short reviews like \"Love it\" can dominate the bag of words without providing much information and context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965617e",
   "metadata": {},
   "source": [
    "### Step 6: Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ccbb76",
   "metadata": {},
   "source": [
    "- What biases may exist in Amazon reviews datasets? In Amazon reviews datasets, there can be selection bias. Those customers that have strongly motivated to write reviews are more likely to be represented in the dataset. Therefore, strong positive or negative reviews may dominate the dataset as opposed to more neutral reviews. Moreover, some newer products may not have had enough time to receive customer reviews and thus may be giving more weight to products that have been in the market for a long time. \n",
    "  \n",
    "- How can text representation choices (BoW vs TF-IDF vs embeddings) impact downstream clustering? For BoW, stopwords or generic praises such as \"Love it\" may appear dominantly and therefore appear as cluster topics. For TF-IDF, it may favor longer reviews since they are likely to introduce more unique words that TF-IDF weighs more heavily on. With Embedding, there can be domain-mismatch between pretrained models and datasets from other domains. This can lead to irrelevant groupings. \n",
    "\n",
    "\n",
    "- Suggest high-level strategies to improve topic discovery (e.g., domain-specific embeddings, metadata, dimensionality reduction). Domain-specific embeddings that works specifically on certain domains can help improve the accuracy of embeddings. Also, including n-grams for TF-IDF can help to improve contextualization. MetadatA integration can be helpful if review ratings are used to preprocess the dataset into positive versus negative reviews. Also, using product categories to divide up the dataset can help topic discovery as well. Principal component analysis may be used for dimensionality reduction. This can help retain the dataset in a manageable size while keeping similar vectors together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f2e79c-7712-49ef-a775-1a2aadcb0a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
