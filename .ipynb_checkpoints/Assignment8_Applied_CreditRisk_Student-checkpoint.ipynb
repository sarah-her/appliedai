{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lnsZXa-JrYax",
   "metadata": {
    "id": "lnsZXa-JrYax"
   },
   "source": [
    "# Assignment 8 – Applied Component: Credit Card Default Prediction using Support Vector Machines (SVM)  \n",
    "**Week 8 Topic:** Advanced Classification with Linear and Non-Linear Kernels  \n",
    "\n",
    "---\n",
    "\n",
    "### Objective  \n",
    "In this assignment, we apply **Support Vector Machines (SVM)** to predict whether a credit-card customer will default on their payment.  \n",
    "You will build both **linear and RBF kernel SVM models**, evaluate their performance, and interpret the results in a financial-risk context.  \n",
    "\n",
    "---\n",
    "\n",
    "### Theoretical Background  \n",
    "\n",
    "#### 1 Support Vector Machines  \n",
    "SVM is a supervised learning algorithm that finds the optimal hyperplane that best separates the data into classes.  \n",
    "It maximizes the margin between the nearest data points of different classes (called support vectors).  \n",
    "\n",
    "**Mathematical Formulation:**  \n",
    "For a binary classification task, SVM solves:  \n",
    "\n",
    "$$\n",
    "\\min_{w,b}\\frac{1}{2}\\|w\\|^2 \\quad\n",
    "\\text{s.t. } y_i(w^Tx_i+b)\\ge1\n",
    "$$  \n",
    "\n",
    "The decision function is:  \n",
    "$$\n",
    "f(x)=\\text{sign}(w^Tx+b)\n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "#### 2 Kernel Trick  \n",
    "Real-world data is often non-linearly separable.  \n",
    "SVM handles this by using **kernel functions** to map data into a higher-dimensional space where a linear separator can be found.  \n",
    "\n",
    "Common kernels:  \n",
    "- Linear Kernel: \\( K(x_i,x_j)=x_i^Tx_j \\)  \n",
    "- Polynomial Kernel: \\( K(x_i,x_j)=(x_i^Tx_j+c)^d \\)  \n",
    "- RBF Kernel: \\( K(x_i,x_j)=\\exp(-\\gamma \\|x_i-x_j\\|^2) \\)  \n",
    "\n",
    "**Linear SVM** works well for high-dimensional text or numeric features.  \n",
    "**RBF SVM** captures complex, non-linear patterns in the data.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 3 Regularization and Hyperparameters  \n",
    "- **C (parameter):** controls the trade-off between margin width and classification errors.  \n",
    "  - Low C → wider margin, more tolerance for errors (high bias).  \n",
    "  - High C → narrow margin, less tolerance for errors (low bias).  \n",
    "- **Gamma (for RBF):** defines how far the influence of a single training example reaches.  \n",
    "  - Low gamma → smooth decision boundary.  \n",
    "  - High gamma → tight fit to training data (overfitting risk).  \n",
    "\n",
    "---\n",
    "\n",
    "#### 4 Evaluation Metrics  \n",
    "\n",
    "| Metric | Formula | Interpretation |  \n",
    "|:--|:--|:--|  \n",
    "| Accuracy | \\( \\frac{TP+TN}{TP+TN+FP+FN} \\) | Overall correctness |  \n",
    "| Precision | \\( \\frac{TP}{TP+FP} \\) | Among predicted defaults, how many were true defaults |  \n",
    "| Recall | \\( \\frac{TP}{TP+FN} \\) | Proportion of actual defaults correctly identified |  \n",
    "| F1 Score | \\( 2 · \\frac{Precision·Recall}{Precision+Recall} \\) | Balance between precision and recall |  \n",
    "| ROC-AUC | Area under ROC curve | Measures class separability |  \n",
    "\n",
    "---\n",
    "\n",
    "### Business Context  \n",
    "\n",
    "Predicting credit-card defaults helps banks identify high-risk customers and reduce losses.  \n",
    "SVM models are robust for financial applications due to their ability to handle high-dimensional and non-linear data.  \n",
    "\n",
    "**Example:**  \n",
    "If a customer has high outstanding balances and irregular payments, an RBF SVM can detect non-linear risk patterns missed by linear models.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26nrEJGHsbVo",
   "metadata": {
    "id": "26nrEJGHsbVo"
   },
   "source": [
    "# Step 1 – Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t1QFoF6GrqKy",
   "metadata": {
    "id": "t1QFoF6GrqKy"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('____')   ### FILL IN BLANK\n",
    "\n",
    "# Display dataset information\n",
    "print(\"Dataset shape:\", ____)   ### FILL IN BLANK\n",
    "print(\"\\nColumns:\\n\", ____)   ### FILL IN BLANK\n",
    "print(\"\\nData Info:\\n\")\n",
    "print(df.info())\n",
    "\n",
    "# Display first few rows\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r7FsAoz6yyUI",
   "metadata": {
    "id": "r7FsAoz6yyUI"
   },
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(df['____'].value_counts())   ### FILL IN BLANK\n",
    "\n",
    "sns.countplot(data=df, x='____')   ### FILL IN BLANK\n",
    "plt.title('Class Distribution (0 = Genuine, 1 = Fraud)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qc1emJpnsZAq",
   "metadata": {
    "id": "Qc1emJpnsZAq"
   },
   "source": [
    "# Step 2: Sampling for Computational Efficiency  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lRDB95xlxLMK",
   "metadata": {
    "collapsed": true,
    "id": "lRDB95xlxLMK",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Perform random sampling for efficiency\n",
    "sample_df = df.sample(n=____, random_state=____)   ### FILL IN BLANK\n",
    "print(\"Sampled Data Shape:\", sample_df.shape)\n",
    "\n",
    "# Check class balance in the sampled data\n",
    "print(sample_df['____'].value_counts(normalize=True))   ### FILL IN BLANK\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BKcRzK_QxTKp",
   "metadata": {
    "id": "BKcRzK_QxTKp"
   },
   "source": [
    "# Step 3: Feature Scaling and Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ILuz6fsixWVt",
   "metadata": {
    "id": "ILuz6fsixWVt"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = sample_df.drop(\"____\", axis=1)   ### FILL IN BLANK\n",
    "y = sample_df[\"____\"]                ### FILL IN BLANK\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(____)   ### FILL IN BLANK\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    ____, ____, test_size=____, random_state=42, stratify=____   ### FILL IN BLANKS\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", X_train.shape[0], \" Testing samples:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wEtnU9eTBwzs",
   "metadata": {
    "id": "wEtnU9eTBwzs"
   },
   "source": [
    "**Interpretation Question:** What might happen if we skip scaling when using distance-based algorithms like SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IdScoUdjxe1W",
   "metadata": {
    "id": "IdScoUdjxe1W"
   },
   "source": [
    "# Step 4: Baseline Linear SVM\n",
    "Using a **linear kernel** with class balancing enabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8Fzz53Eixils",
   "metadata": {
    "id": "8Fzz53Eixils"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "linear_svm = SVC(kernel='____', class_weight='____', probability=True, random_state=42)   ### FILL IN BLANKS\n",
    "linear_svm.fit(____, ____)   ### FILL IN BLANK\n",
    "y_pred_linear = linear_svm.predict(____)   ### FILL IN BLANK\n",
    "\n",
    "print(\"\\n--- Linear Kernel SVM ---\")\n",
    "print(classification_report(____, ____, digits=4))   ### FILL IN BLANK\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(____, linear_svm.predict_proba(____)[:,1]))   ### FILL IN BLANKS\n",
    "\n",
    "cm_linear = confusion_matrix(____, ____)   ### FILL IN BLANK\n",
    "print(\"\\nConfusion Matrix:\\n\", cm_linear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020Pf_RmB5Ak",
   "metadata": {
    "id": "020Pf_RmB5Ak"
   },
   "source": [
    "**Interpretation Question:** Why might a linear boundary perform poorly on complex, nonlinear data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_0jb3cVkxrc_",
   "metadata": {
    "id": "_0jb3cVkxrc_"
   },
   "source": [
    "# Step 5: Faster RBF SVM (No Grid Search)\n",
    "Train an **RBF kernel SVM**, which is capable of handling non-linear relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0HiOCyjuxwbw",
   "metadata": {
    "id": "0HiOCyjuxwbw"
   },
   "outputs": [],
   "source": [
    "rbf_svm = SVC(kernel='____', C=____, gamma='____', class_weight='____', probability=True, random_state=42)   ### FILL IN BLANKS\n",
    "rbf_svm.fit(____, ____)   ### FILL IN BLANK\n",
    "y_pred_rbf = rbf_svm.predict(____)   ### FILL IN BLANK\n",
    "\n",
    "print(\"\\n--- RBF Kernel SVM ---\")\n",
    "print(classification_report(____, ____, digits=4))   ### FILL IN BLANK\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(____, rbf_svm.predict_proba(____)[:,1]))   ### FILL IN BLANKS\n",
    "\n",
    "cm_rbf = confusion_matrix(____, ____)   ### FILL IN BLANK\n",
    "print(\"\\nConfusion Matrix:\\n\", cm_rbf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2j2qnDm6CDfN",
   "metadata": {
    "id": "2j2qnDm6CDfN"
   },
   "source": [
    "**Interpretation Question:** How does the RBF kernel differ from the linear kernel in handling nonlinear relationships?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exwNAFfOx1-Q",
   "metadata": {
    "id": "exwNAFfOx1-Q"
   },
   "source": [
    "# Step 6: Compare Model Performances\n",
    "Evaluate and compare Linear vs RBF SVMs using key metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Uk1zhi0xx6ag",
   "metadata": {
    "id": "Uk1zhi0xx6ag"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Linear SVM\", \"RBF SVM\"],\n",
    "    \"Accuracy\": [\n",
    "        accuracy_score(____, ____),   ### FILL IN BLANK\n",
    "        accuracy_score(____, ____)    ### FILL IN BLANK\n",
    "    ],\n",
    "    \"F1 Score\": [\n",
    "        f1_score(____, ____),         ### FILL IN BLANK\n",
    "        f1_score(____, ____)          ### FILL IN BLANK\n",
    "    ],\n",
    "    \"ROC-AUC\": [\n",
    "        roc_auc_score(____, linear_svm.predict_proba(____)[:,1]),   ### FILL IN BLANK\n",
    "        roc_auc_score(____, rbf_svm.predict_proba(____)[:,1])       ### FILL IN BLANK\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n--- Model Performance Summary ---\")\n",
    "display(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "letdpqLzCRuW",
   "metadata": {
    "id": "letdpqLzCRuW"
   },
   "source": [
    "**Interpretation Question:** Which model shows the highest accuracy or F1 score, and what does that imply?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tv0Dh2anx-iE",
   "metadata": {
    "id": "tv0Dh2anx-iE"
   },
   "source": [
    "# Step 7: Visualize ROC Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kpo5FBh3yZ5F",
   "metadata": {
    "id": "kpo5FBh3yZ5F"
   },
   "outputs": [],
   "source": [
    "y_scores = ____.decision_function(____)   ### FILL IN BLANK\n",
    "precision, recall, _ = precision_recall_curve(____, ____)   ### FILL IN BLANK\n",
    "average_precision = average_precision_score(____, ____)     ### FILL IN BLANK\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(____, ____, label=f'RBF SVM (AP={average_precision:.4f})')   ### FILL IN BLANK\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ylEiW2R9yAdm",
   "metadata": {
    "id": "ylEiW2R9yAdm"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr_lin, tpr_lin, _ = roc_curve(____, ____.predict_proba(____)[:,1])   ### FILL IN BLANK\n",
    "fpr_rbf, tpr_rbf, _ = roc_curve(____, ____.predict_proba(____)[:,1])   ### FILL IN BLANK\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(____, ____, label=f\"Linear SVM (AUC = {auc(____, ____):.3f})\")   ### FILL IN BLANK\n",
    "plt.plot(____, ____, label=f\"RBF SVM (AUC = {auc(____, ____):.3f})\")     ### FILL IN BLANK\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison: Linear vs RBF SVM\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6TmOvFRYzmWW",
   "metadata": {
    "id": "6TmOvFRYzmWW"
   },
   "source": [
    "# Step 8: Reflection and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uxw2Rn1cy4oD",
   "metadata": {
    "id": "uxw2Rn1cy4oD"
   },
   "source": [
    "### 1. What does the Precision–Recall curve reveal about model behavior on imbalanced data?Discuss its shape and why Average Precision is preferred over ROC AUC in extreme imbalance.\n",
    "\n",
    "### 2. Compare Linear vs RBF kernel results: Which performs better in Recall? Why? Relate to data non-linearity and model complexity.\n",
    "\n",
    "### 3. Explain how class_weight='balanced' affects the SVM optimization objective and margin position.\n",
    "\n",
    "### 4. If false negatives are costlier than false positives, how would you adjust this pipeline? Consider threshold tuning, custom loss functions, or cost-sensitive learning.\n",
    "\n",
    "### 5. Would you deploy this SVM in real-time fraud detection? Why or why not? Think about latency, interpretability, and update frequency vs Gradient Boosting or Deep Models."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
